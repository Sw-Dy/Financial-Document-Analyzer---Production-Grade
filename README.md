# Financial Document Analyzer - Production Grade

**Status:** âœ… **PRODUCTION READY**  
**Version:** 2.0.0  
**Last Updated:** February 27, 2026

---

## ğŸ“‹ Quick Overview

Advanced financial document analysis system that processes corporate reports, financial statements, and investment documents using CrewAI agents with enterprise-grade architecture.

**Key Features:**
- âœ… Non-blocking async API with Celery task queue
- âœ… SQLite database for task persistence
- âœ… Four specialized financial analysis agents
- âœ… Structured JSON output format
- âœ… Full audit trail with timestamps
- âœ… Development mode (no Redis required)
- âœ… Production mode with Redis support
- âœ… Comprehensive logging and error handling

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Server                           â”‚
â”‚  (Non-blocking HTTP endpoints with async/await)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼           â–¼           â–¼
    POST /analyze   GET /status  GET /analysis
    (submit)        (check)      (retrieve)
         â”‚           â”‚           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   SQLite Database     â”‚
         â”‚  (Persistent Storage) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Celery Workers   â”‚   â”‚ CrewAI Agents         â”‚
â”‚                  â”‚   â”‚ (Background Tasks)    â”‚
â”‚  Development:    â”‚   â”‚                       â”‚
â”‚  SQLite Broker   â”‚   â”‚ â€¢ Senior Analyst      â”‚
â”‚                  â”‚   â”‚ â€¢ Risk Assessor       â”‚
â”‚  Production:     â”‚   â”‚ â€¢ Cash Flow Expert    â”‚
â”‚  Redis Broker    â”‚   â”‚ â€¢ Investment Strategy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Getting Started (5 Minutes)

### Step 1: Install Dependencies

```bash
# Navigate to project directory
cd financial-document-analyzer-debug

# Install all required packages
pip install -r requirements.txt

# (Automatically installs Celery, SQLAlchemy, FastAPI, etc.)
```

### Step 2: Start FastAPI Server

**Terminal 1:**
```bash
uvicorn main:app --reload
```

Output:
```
INFO:     Uvicorn running on http://127.0.0.1:8000
âœ“ Database initialized
âœ“ Directories created
âœ“ Application startup complete
```

### Step 3: Start Celery Worker

**Terminal 2:**
```bash
celery -A celery_app worker --loglevel=info --pool=solo
```

Output:
```
[Celery] Using SQLAlchemy/SQLite backend (development mode - no Redis required)
celery@YOUR-MACHINE ready.
```

### Step 4: Test the API

**Terminal 3:**
```bash
# Submit a PDF for analysis
curl -X POST http://localhost:8000/analyze \
  -F "file=@financial_document.pdf" \
  -F "query=Comprehensive financial analysis"

# Response:
{
  "task_id": "12345-uuid-6789",
  "status": "pending",
  "status_url": "/status/12345-uuid-6789",
  "message": "Analysis submitted successfully"
}

# Check status
curl http://localhost:8000/status/12345-uuid-6789

# Get results (after completion)
curl http://localhost:8000/analysis/12345-uuid-6789
```

---

## ğŸ“¡ API Documentation

### Endpoints

#### 1. **Health Check** âœ“

```
GET /health
```

**Response (200 OK):**
```json
{
  "status": "healthy",
  "service": "financial-analyzer",
  "version": "2.0.0",
  "timestamp": "2026-02-27T03:20:00Z",
  "components": {
    "api": "operational",
    "database": "operational",
    "celery": "operational"
  }
}
```

---

#### 2. **Submit Analysis** (Non-blocking)

```
POST /analyze
Content-Type: multipart/form-data
```

**Parameters:**
| Field | Type | Required | Description |
|-------|------|----------|-------------|
| file | File | Yes | PDF financial document |
| query | String | No | Analysis instructions (default: "Analyze this financial document comprehensively") |
| email | String | No | User email for tracking |

**Example:**
```bash
curl -X POST http://localhost:8000/analyze \
  -F "file=@annual_report.pdf" \
  -F "query=Analyze revenue trends and profitability" \
  -F "email=analyst@company.com"
```

**Response (200 OK):**
```json
{
  "task_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "status": "pending",
  "message": "Analysis submitted successfully. Use status endpoint to check progress.",
  "status_url": "/status/a1b2c3d4-e5f6-7890-abcd-ef1234567890"
}
```

**Use `task_id` for all subsequent requests.**

---

#### 3. **Check Status** (Polling)

```
GET /status/{task_id}
```

**Path Parameter:**
- `task_id`: UUID string from /analyze response

**Example:**
```bash
curl http://localhost:8000/status/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

**Response (200 OK):**
```json
{
  "task_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "status": "running",
  "progress": 50
}
```

**Status Values:**
- `pending` (0%) - Queued, not yet started
- `running` (50%) - Analysis in progress
- `completed` (100%) - Finished successfully
- `failed` (0%) - Error occurred

**Retry Logic:**
```python
import time
task_id = "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
while True:
    response = requests.get(f"http://localhost:8000/status/{task_id}")
    status = response.json()["status"]
    
    if status == "completed":
        print("Analysis ready! Call /analysis endpoint")
        break
    elif status == "failed":
        print("Analysis failed")
        break
    else:
        print(f"Status: {status}")
        time.sleep(2)  # Poll every 2 seconds
```

---

#### 4. **Retrieve Result** (When Complete)

```
GET /analysis/{task_id}
```

**Path Parameter:**
- `task_id`: UUID string from /analyze response

**Example:**
```bash
curl http://localhost:8000/analysis/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

**Response (200 OK - Completed):**
```json
{
  "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": 1,
  "filename": "annual_report.pdf",
  "status": "completed",
  "result": {
    "revenue_analysis": "Revenue grew 15% YoY from $1.2B to $1.38B...",
    "profitability_analysis": "Net margins improved from 8% to 10%...",
    "cash_flow_analysis": "Operating cash flow increased 20% YoY...",
    "risk_assessment": "Primary risks: competitive pressure, regulatory...",
    "recommendation": "BUY",
    "confidence_score": 87,
    "cited_sources": [
      "Page 2: Revenue breakdown by segment",
      "Page 5: Profit & loss statement"
    ],
    "reasoning": "Strong fundamentals with improving margins justify buy recommendation..."
  },
  "error": null,
  "created_at": "2026-02-27T03:15:00Z",
  "completed_at": "2026-02-27T03:20:30Z"
}
```

**Response (202 - Still Processing):**
```json
{
  "detail": "Analysis still processing. Current status: running. Check /status/{task_id}"
}
```

**Response (400 - Failed):**
```json
{
  "detail": "Analysis failed: PDF parsing error - corrupted file"
}
```

**Response (404 - Not Found):**
```json
{
  "detail": "Analysis {task_id} not found"
}
```

---

## ğŸ”§ Configuration

### Environment Variables

Create `.env` file in project root:

```bash
# Production mode only
ENV=production
REDIS_URL=redis://localhost:6379/0

# Optional: Database
DATABASE_URL=sqlite:///./financial_analyzer.db

# Optional: OpenAI
OPENAI_API_KEY=sk-...
```

### Development Mode (Default)

No environment variables needed. Runs with:
- SQLite broker/backend (auto-created in `celery_data/`)
- Dummy LLM fallback (no API key required)
- Debug logging enabled

### Production Mode

```bash
export ENV=production
export REDIS_URL=redis://your-redis-server:6379/0
```

---

## ğŸ“Š Database Schema

### Users Table

```sql
CREATE TABLE users (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  email VARCHAR(255) UNIQUE NOT NULL,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### Analysis Results Table

```sql
CREATE TABLE analysis_results (
  id VARCHAR(36) PRIMARY KEY,           -- UUID (task_id)
  user_id INTEGER FOREIGN KEY,
  filename VARCHAR(255) NOT NULL,
  status VARCHAR(50) NOT NULL,          -- pending|running|completed|failed
  result_json JSON,                     -- Structured analysis output
  error_message VARCHAR(500),
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  completed_at DATETIME
);
```

---

## ğŸ“ Analysis Output Format

All results follow this JSON schema:

```json
{
  "revenue_analysis": "string (detailed revenue metrics)",
  "profitability_analysis": "string (margin analysis, ROE, etc)",
  "cash_flow_analysis": "string (operating CF, FCF, capex)",
  "risk_assessment": "string (liquidity, solvency, operational risks)",
  "recommendation": "BUY|HOLD|SELL",
  "confidence_score": 0-100,
  "cited_sources": ["string", "string"],
  "reasoning": "string (justification for recommendation)"
}
```

**Constraints:**
- All fields use documentary evidence from PDF
- Recommendations require â‰¥70% confidence to be issued
- All claims must cite specific document sections
- Temperature set to 0.2 for consistency

---

## ğŸ› Debugging & Troubleshooting

### View Logs

**FastAPI:**
```bash
# Terminal running uvicorn shows all logs
# Look for [Endpoint], [Analyze], [Status] markers
```

**Celery:**
```bash
# Terminal running Celery shows all logs
# Look for [Task], [CRUD] markers
```

**Database:**
```bash
# Check current tasks
sqlite3 financial_analyzer.db
sqlite> SELECT id, status, created_at FROM analysis_results;
```

### Common Issues

#### 1. Celery "Connection refused" error

**Symptom:**
```
Error 10061 connecting to localhost:6379
```

**Fix:** Celery is configured to use SQLite in development mode. Ensure:
```bash
# Environment should NOT be production
echo $ENV  # Should be empty or "development"

# Restart Celery
celery -A celery_app worker --loglevel=info --pool=solo
```

#### 2. 404 "Task not found"

**Symptom:**
```
GET /status/{task_id} returns 404
```

**Fix:** Ensure:
1. Session is properly closed (FastAPI dependency management)
2. Record was committed to database
3. Check logs for `âœ“ VERIFIED` message

**Verify:**
```bash
# Check if record exists in database
sqlite3 financial_analyzer.db
sqlite> SELECT * FROM analysis_results WHERE id = 'YOUR_TASK_ID';
```

#### 3. File upload fails

**Symptom:**
```
400 Bad Request: Only PDF files are supported
```

**Fix:** 
- Ensure file has `.pdf` extension
- Maximum file size: System RAM available
- Example:
```bash
curl -X POST http://localhost:8000/analyze \
  -F "file=@report.pdf"  # Must be .pdf
```

#### 4. Analysis takes too long

**Symptom:**
```
GET /analysis/{task_id} still returns 202 after 10+ minutes
```

**Fix:**
- Check Celery logs for CrewAI execution
- Verify PDF is readable and not corrupted
- Check OPENAI_API_KEY if using real LLM (development uses dummy)

---

## ğŸ“š Complete Bug Fixes Reference

**This project had 9 critical bugs that have been fixed:**

| Bug # | Issue | Severity | Status |
|-------|-------|----------|--------|
| #1 | Incorrect CrewAI import path | CRITICAL | âœ… FIXED |
| #2 | Circular LLM self-reference | CRITICAL | âœ… FIXED |
| #3 | Invalid tool type in Task | CRITICAL | âœ… FIXED |
| #4 | Wrong tool decorator import | CRITICAL | âœ… FIXED |
| #5 | Missing PDF loader import | CRITICAL | âœ… FIXED |
| #6 | Pydantic v2 EmailStr type | CRITICAL | âœ… FIXED |
| #7 | MultipartForm missing dep | HIGH | âœ… FIXED |
| #8 | CrewAI Agent memory conflicts | HIGH | âœ… FIXED |
| #9 | Session lifecycle (404 errors) | CRITICAL | âœ… FIXED |

**For detailed explanations, see [DEBUG_REPORT.md](debug_report.md)**

---

## ğŸš€ Production Deployment

### Docker Deployment

```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

ENV ENV=production
ENV REDIS_URL=redis://redis:6379/0

CMD ["gunicorn", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "main:app"]
```

### Docker Compose

```yaml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENV=production
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis

  celery:
    build: .
    command: celery -A celery_app worker --loglevel=info
    environment:
      - ENV=production
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis

  celery-flower:
    build: .
    command: celery -A celery_app flower
    ports:
      - "5555:5555"
    environment:
      - ENV=production
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
```

Run with:
```bash
docker-compose up -d
# API: http://localhost:8000
# Flower: http://localhost:5555
```

### Environment-Specific Configuration

**Development:**
```bash
ENV=development
# Automatically uses SQLite
# No Redis required
# Dummy LLM fallback
```

**Production:**
```bash
ENV=production
REDIS_URL=redis://prod-redis:6379/0
OPENAI_API_KEY=sk-prod-xxxx
```

---

## ğŸ“Š Monitoring

### Task Queue Status

Visit Celery Flower UI:
```bash
# Terminal 4:
celery -A celery_app flower
# Open http://localhost:5555
```

**View:**
- Active tasks
- Success/failure rates
- Task execution times
- Worker status

### Database Queries

```bash
# All pending tasks
sqlite3 financial_analyzer.db
sqlite> SELECT id, status, created_at FROM analysis_results WHERE status='pending';

# Failed tasks with errors
sqlite> SELECT id, error_message FROM analysis_results WHERE status='failed';

# Completed tasks with timestamps
sqlite> SELECT id, completed_at, datetime(completed_at '-created_at') as duration 
        FROM analysis_results WHERE status='completed';
```

---

## ğŸ“¦ Project Structure

```
financial-document-analyzer-debug/
â”œâ”€â”€ main.py                    # FastAPI endpoints (POST /analyze, GET /status, GET /analysis)
â”œâ”€â”€ celery_app.py             # Celery configuration (development/production toggle)
â”œâ”€â”€ agents.py                 # 4 production financial analysis agents
â”œâ”€â”€ task.py                   # CrewAI task definition
â”œâ”€â”€ crud.py                   # Database CRUD operations
â”œâ”€â”€ db.py                     # SQLAlchemy ORM setup, session management
â”œâ”€â”€ models.py                 # User and AnalysisResult models
â”œâ”€â”€ schemas.py                # Pydantic v2 request/response schemas
â”œâ”€â”€ tools.py                  # PDF processing and utilities
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ debug_report.md          # Detailed bug explanations and fixes
â”œâ”€â”€ test_db_flow.py          # Database session lifecycle test
â”‚
â”œâ”€â”€ data/                     # Input PDF storage
â”œâ”€â”€ outputs/                  # Analysis output storage
â”œâ”€â”€ celery_data/             # SQLite Celery broker/results (auto-created)
â””â”€â”€ __pycache__/             # Python cache
```

---

## ğŸ§ª Testing

### Unit Test: Database Session Lifecycle

```bash
python test_db_flow.py
```

Output:
```
âœ“ Record created: status=pending
âœ“ Record found after fresh session
âœ“ Record updated successfully
âœ“ SUCCESS: Database flow working correctly!
```

---

## ğŸ” Security Considerations

- SQLite database file contains full analysis history
- Backup database regularly in production
- Use environment variables for API keys
- Implement authentication/authorization before production
- Validate file uploads (PDF type, max size)
- Rate limit API endpoints if exposed publicly

---

## ğŸ“ Support & Documentation

| Document | Purpose |
|----------|---------|
| [DEBUG_REPORT.md](debug_report.md) | Complete bug analysis and fixes |
| `test_db_flow.py` | Database layer testing |
| Logs | Real-time execution tracking |
| `/docs` | Interactive API documentation (Swagger UI) |
| `/redoc` | Alternative API documentation |

---

## âœ… Production Readiness Checklist

- âœ… All 9 critical bugs fixed
- âœ… Database session lifecycle corrected (no more 404 errors)
- âœ… Non-blocking async API with Celery queue
- âœ… SQLAlchemy ORM for persistent storage
- âœ… Four production-grade financial agents
- âœ… Comprehensive error handling and logging
- âœ… Development mode (no Redis required)
- âœ… Production mode with Redis support
- âœ… Docker deployment ready
- âœ… Monitoring capabilities (Flower, Logs)
- âœ… Complete API documentation

---

**System Status:** ğŸŸ¢ **PRODUCTION READY**  
**Last Tested:** February 27, 2026  
**Version:** 2.0.0
